svmModelLinear <- svm(Select ~ ., data = train, type = "C-classification", cost = 1, kernel = kernels[i])
predictions <- predict(svmModelLinear, test)
myTable <- table(testCheck$"Select", predictions)
errorList[i] <- (myTable[2] + myTable[3]) / sum(myTable)
}
return(kernels[unlist(which.min(errorList))])
}
getBestKernel2 <- function(train, testCheck, kernels){
test <- testCheck[,-10]
errorList <- list()
for (i in 1:length(kernels)){
svmModelLinear <- svm(Select ~ ., data = train, type = "C-classification", cost = 1, kernel = kernels[i])
predictions <- predict(svmModelLinear, test)
myTable <- table(testCheck$"Select", predictions)
errorList[i] <- (myTable[2] + myTable[3]) / sum(myTable)
}
return(errorList)
}
getBestMFinalBagging <- function(train,test,mfinal){
resList <- list()
for (i in 1:length(mfinal)) {
model <- bagging(Select ~.,data = train, mfinal = mfinal[i], maxdepth = 5)
predictions <- predict.bagging(model, newdata = test)
resList[i] <- predictions$error
}
return(resList)
}
getBestMFinalBoosting <- function(train,test,mfinal){
resList <- list()
for (i in 1:length(mfinal)) {
model <- boosting(Select ~.,data = train, mfinal = mfinal[i], maxdepth = 5)
predictions <- predict.boosting(model, newdata = test)
resList[i] <- predictions$error
}
return(resList)
}
############################################################################################
##Visual
############################################################################################
SourceData <- SourceData[,-10]
n <- dim(SourceData)[1]
idx <- sample(1:n, n*0.7)
trainData <- SourceData[idx, ]
testData <- SourceData[-idx, ]
train_data <- unique(trainData)
train_data$Sex <- as.numeric(as.factor(train_data$Sex))
tsne <- Rtsne(as.matrix(train_data))
plot(tsne$Y,pch = 22, bg = c("blue","yellow"))
############################################################################################
##SVM
############################################################################################
#Нашли ядро с наименьшей ошибкой
kernel <- getBestKernel(trainData,testData,c("linear","radial","sigmoid","polynomial"))
#kkkkkk <- getBestKernel2(trainData,testData,c("linear","radial","sigmoid","polynomial"))
svmModelLinear <- svm(Select ~ ., data = trainData, type = "C-classification", cost = 1, kernel = kernel)
predictions <- predict(svmModelLinear, testData[,-10])
myTable <- table(testData$"Select", predictions)
errSvm <- (myTable[2] + myTable[3]) / sum(myTable)
############################################################################################
##BAGGING
############################################################################################
mfinal <- seq(1,6,5)
trainData$Select <- as.factor(trainData$Select)
resList <- getBestMFinalBagging(trainData,testData,mfinal)
plot(mfinal,resList,
col = "red",
xlab = "Number of trees",
ylab = "Classification error")
#Нашли количество деревьев с наименьшей ошибкой
treeNumberBag <- mfinal[unlist(which.min(resList))]
baggingModel <- bagging(Select ~.,data = trainData, mfinal = treeNumberBag, maxdepth = 5)
predictions <- predict.bagging(baggingModel, newdata = testData)
errBagging <- predictions$error
############################################################################################
##BOOSTING
############################################################################################
resList <- getBestMFinalBoosting(trainData,testData,mfinal)
plot(mfinal,resList,
col = "red",
xlab = "Number of trees",
ylab = "Classification error")
#Нашли количество деревьев с наименьшей ошибкой
treeNumberBoost <- mfinal[unlist(which.min(resList))]
boostingModel <- boosting(Select ~.,data = trainData, mfinal = treeNumberBoost, maxdepth = 5)
predictions <- predict.boosting(boostingModel, newdata = testData)
errBoosting <- predictions$error
############################################################################################
##CLUSTER
############################################################################################
cl <- clara(SourceData[,-10], 2,metric = c("euclidean"))
t <- table(cl$clustering,SourceData$Select)
errClara <- sum(diag(t))/sum(t)
plot(cl)
data$Sex <- as.numeric(as.factor(data$Sex))
kkk <- kmeans(data[,-10],2,iter.max = 10)
t <- table(kkk$cluster,SourceData$Select)
errKMeans <- sum(diag(t))/sum(t)
dendogramma <- agnes(SourceData[,-10])
plot(dendogramma)
hc = hclust(dist(SourceData[,-10]))
# very simple dendrogram
plot(hc)
plot(hc,hang=-1)
dendogramma <- agnes(SourceData[,-10])
plot(dendogramma)
groups<-cutree(hc,k=2)
rect.hclust(hc,k=2,border = "red")
t<-table(groups,diabetes[,9])
(sum(diag(t))/sum(t))
hc = hclust(dist(SourceData[,-10]))
# very simple dendrogram
plot(hc,hang=-1)
groups<-cutree(hc,k=2)
#rect.hclust(hc,k=2,border = "red")
t<-table(groups,diabetes[,9])
(sum(diag(t))/sum(t))
cl <- clara(SourceData[,-10], 2,metric = c("euclidean"))
t <- table(cl$clustering,SourceData$Select)
errClara <- sum(diag(t))/sum(t)
plot(cl)
data$Sex <- as.numeric(as.factor(data$Sex))
kkk <- kmeans(data[,-10],2,iter.max = 10)
t <- table(kkk$cluster,SourceData$Select)
errKMeans <- sum(diag(t))/sum(t)
# prepare hierarchical cluster
hc = hclust(dist(SourceData[,-10]))
# very simple dendrogram
plot(hc,hang=-1)
groups<-cutree(hc,k=2)
#rect.hclust(hc,k=2,border = "red")
t<-table(groups,diabetes[,9])
(sum(diag(t))/sum(t))
t<-table(groups,SourceData[,10])
(sum(diag(t))/sum(t))
x <- as.matrix(data[,-10])
y <- data[,10]
glm = glmnet(x, y, family = "binomial", alpha = 1)
lassoResult <- as.matrix(glm$beta)
library(e1071)
library(adabag)
library(rpart)
library(Rtsne)
library(cluster)
library(glmnet)
library(autoencoder)
SourceData <- read.csv("C:/Users/anna_fox/Documents/UNIVERSE/4 курс/2 semester/Нейронные_сети/All_Labs/Kursovoj/Indian Liver Patient Dataset (ILPD).csv",stringsAsFactors = TRUE)
getBestKernel <- function(train, testCheck, kernels){
test <- testCheck[,-10]
errorList <- list()
for (i in 1:length(kernels)){
svmModelLinear <- svm(Select ~ ., data = train, type = "C-classification", cost = 1, kernel = kernels[i])
predictions <- predict(svmModelLinear, test)
myTable <- table(testCheck$"Select", predictions)
errorList[i] <- (myTable[2] + myTable[3]) / sum(myTable)
}
return(kernels[unlist(which.min(errorList))])
}
getBestKernel2 <- function(train, testCheck, kernels){
test <- testCheck[,-10]
errorList <- list()
for (i in 1:length(kernels)){
svmModelLinear <- svm(Select ~ ., data = train, type = "C-classification", cost = 1, kernel = kernels[i])
predictions <- predict(svmModelLinear, test)
myTable <- table(testCheck$"Select", predictions)
errorList[i] <- (myTable[2] + myTable[3]) / sum(myTable)
}
return(errorList)
}
getBestMFinalBagging <- function(train,test,mfinal){
resList <- list()
for (i in 1:length(mfinal)) {
model <- bagging(Select ~.,data = train, mfinal = mfinal[i], maxdepth = 5)
predictions <- predict.bagging(model, newdata = test)
resList[i] <- predictions$error
}
return(resList)
}
getBestMFinalBoosting <- function(train,test,mfinal){
resList <- list()
for (i in 1:length(mfinal)) {
model <- boosting(Select ~.,data = train, mfinal = mfinal[i], maxdepth = 5)
predictions <- predict.boosting(model, newdata = test)
resList[i] <- predictions$error
}
return(resList)
}
############################################################################################
##Visual
############################################################################################
SourceData <- SourceData[,-10]
n <- dim(SourceData)[1]
idx <- sample(1:n, n*0.7)
trainData <- SourceData[idx, ]
testData <- SourceData[-idx, ]
train_data <- unique(trainData)
train_data$Sex <- as.numeric(as.factor(train_data$Sex))
tsne <- Rtsne(as.matrix(train_data))
plot(tsne$Y,pch = 22, bg = c("blue","yellow"))
############################################################################################
##SVM
############################################################################################
#Нашли ядро с наименьшей ошибкой
kernel <- getBestKernel(trainData,testData,c("linear","radial","sigmoid","polynomial"))
#kkkkkk <- getBestKernel2(trainData,testData,c("linear","radial","sigmoid","polynomial"))
svmModelLinear <- svm(Select ~ ., data = trainData, type = "C-classification", cost = 1, kernel = kernel)
predictions <- predict(svmModelLinear, testData[,-10])
myTable <- table(testData$"Select", predictions)
errSvm <- (myTable[2] + myTable[3]) / sum(myTable)
############################################################################################
##BAGGING
############################################################################################
mfinal <- seq(1,6,5)
trainData$Select <- as.factor(trainData$Select)
resList <- getBestMFinalBagging(trainData,testData,mfinal)
plot(mfinal,resList,
col = "red",
xlab = "Number of trees",
ylab = "Classification error")
#Нашли количество деревьев с наименьшей ошибкой
treeNumberBag <- mfinal[unlist(which.min(resList))]
baggingModel <- bagging(Select ~.,data = trainData, mfinal = treeNumberBag, maxdepth = 5)
predictions <- predict.bagging(baggingModel, newdata = testData)
errBagging <- predictions$error
############################################################################################
##BOOSTING
############################################################################################
resList <- getBestMFinalBoosting(trainData,testData,mfinal)
plot(mfinal,resList,
col = "red",
xlab = "Number of trees",
ylab = "Classification error")
#Нашли количество деревьев с наименьшей ошибкой
treeNumberBoost <- mfinal[unlist(which.min(resList))]
boostingModel <- boosting(Select ~.,data = trainData, mfinal = treeNumberBoost, maxdepth = 5)
predictions <- predict.boosting(boostingModel, newdata = testData)
errBoosting <- predictions$error
############################################################################################
##CLUSTER
############################################################################################
cl <- clara(SourceData[,-10], 2,metric = c("euclidean"))
t <- table(cl$clustering,SourceData$Select)
errClara <- sum(diag(t))/sum(t)
plot(cl)
data$Sex <- as.numeric(as.factor(data$Sex))
kkk <- kmeans(data[,-10],2,iter.max = 10)
t <- table(kkk$cluster,SourceData$Select)
errKMeans <- sum(diag(t))/sum(t)
# prepare hierarchical cluster
hc = hclust(dist(SourceData[,-10]))
# very simple dendrogram
plot(hc,hang=-1)
groups<-cutree(hc,k=2)
#rect.hclust(hc,k=2,border = "red")
t<-table(groups,SourceData[,10])
(sum(diag(t))/sum(t))
############################################################################################
##LASSO
############################################################################################
x <- as.matrix(data[,-10])
y <- data[,10]
glm = glmnet(x, y, family = "binomial", alpha = 1)
lassoResult <- as.matrix(glm$beta)
library(e1071)
library(adabag)
library(rpart)
library(Rtsne)
library(cluster)
library(glmnet)
library(autoencoder)
SourceData <- read.csv("C:/Users/anna_fox/Documents/UNIVERSE/4 курс/2 semester/Нейронные_сети/All_Labs/Kursovoj/Indian Liver Patient Dataset (ILPD).csv",stringsAsFactors = TRUE)
getBestKernel <- function(train, testCheck, kernels){
test <- testCheck[,-10]
errorList <- list()
for (i in 1:length(kernels)){
svmModelLinear <- svm(Select ~ ., data = train, type = "C-classification", cost = 1, kernel = kernels[i])
predictions <- predict(svmModelLinear, test)
myTable <- table(testCheck$"Select", predictions)
errorList[i] <- (myTable[2] + myTable[3]) / sum(myTable)
}
return(kernels[unlist(which.min(errorList))])
}
getBestKernel2 <- function(train, testCheck, kernels){
test <- testCheck[,-10]
errorList <- list()
for (i in 1:length(kernels)){
svmModelLinear <- svm(Select ~ ., data = train, type = "C-classification", cost = 1, kernel = kernels[i])
predictions <- predict(svmModelLinear, test)
myTable <- table(testCheck$"Select", predictions)
errorList[i] <- (myTable[2] + myTable[3]) / sum(myTable)
}
return(errorList)
}
getBestMFinalBagging <- function(train,test,mfinal){
resList <- list()
for (i in 1:length(mfinal)) {
model <- bagging(Select ~.,data = train, mfinal = mfinal[i], maxdepth = 5)
predictions <- predict.bagging(model, newdata = test)
resList[i] <- predictions$error
}
return(resList)
}
getBestMFinalBoosting <- function(train,test,mfinal){
resList <- list()
for (i in 1:length(mfinal)) {
model <- boosting(Select ~.,data = train, mfinal = mfinal[i], maxdepth = 5)
predictions <- predict.boosting(model, newdata = test)
resList[i] <- predictions$error
}
return(resList)
}
############################################################################################
##Visual
############################################################################################
SourceData <- SourceData[,-10]
n <- dim(SourceData)[1]
idx <- sample(1:n, n*0.7)
trainData <- SourceData[idx, ]
testData <- SourceData[-idx, ]
train_data <- unique(trainData)
train_data$Sex <- as.numeric(as.factor(train_data$Sex))
tsne <- Rtsne(as.matrix(train_data))
plot(tsne$Y,pch = 22, bg = c("blue","yellow"))
############################################################################################
##SVM
############################################################################################
#Нашли ядро с наименьшей ошибкой
kernel <- getBestKernel(trainData,testData,c("linear","radial","sigmoid","polynomial"))
#kkkkkk <- getBestKernel2(trainData,testData,c("linear","radial","sigmoid","polynomial"))
svmModelLinear <- svm(Select ~ ., data = trainData, type = "C-classification", cost = 1, kernel = kernel)
predictions <- predict(svmModelLinear, testData[,-10])
myTable <- table(testData$"Select", predictions)
errSvm <- (myTable[2] + myTable[3]) / sum(myTable)
############################################################################################
##BAGGING
############################################################################################
mfinal <- seq(1,6,5)
trainData$Select <- as.factor(trainData$Select)
resList <- getBestMFinalBagging(trainData,testData,mfinal)
data <- SourceData
data$Sex <- as.numeric(as.factor(data$Sex))
data$Sex <- as.numeric(as.factor(data$Sex))
kkk <- kmeans(data[,-10],2,iter.max = 10)
t <- table(kkk$cluster,SourceData$Select)
errKMeans <- sum(diag(t))/sum(t)
# prepare hierarchical cluster
hc = hclust(dist(SourceData[,-10]))
# very simple dendrogram
plot(hc,hang=-1)
groups<-cutree(hc,k=2)
#rect.hclust(hc,k=2,border = "red")
t<-table(groups,SourceData[,10])
(sum(diag(t))/sum(t))
############################################################################################
##LASSO
############################################################################################
x <- as.matrix(data[,-10])
y <- data[,10]
glm = glmnet(x, y, family = "binomial", alpha = 1)
lassoResult <- as.matrix(glm$beta)
View(lassoResult)
View(lassoResult)
testData$Sex <- as.numeric(as.factor(testData$Sex))
trainData$Sex <- as.numeric(as.factor(trainData$Sex))
encoder <- autoencode(as.matrix(data[,-10]),
N.hidden = 15,
epsilon = 0,
lambda = 0.0002,
beta = 6,
rho = 0.9,
unit.type = "tanh",
rescale.flag = TRUE)
View(encoder)
View(encoder)
predictions <- predict.autoencoder(encoder,as.matrix(data[,-10]),hidden.output = TRUE)
predFrame <- as.data.frame(predictions$X.output)
autoEncodedData <- tibble::add_column(predFrame,data$Select)
tsne <- Rtsne(as.matrix(unique(autoEncodedData)),perplexity = 20)
plot(tsne$Y,pch = 22, bg = c("blue","yellow"))
View(autoEncodedData)
View(autoEncodedData)
View(predictions)
View(predictions)
predictions[["X.output"]]
View(SourceData)
View(SourceData)
View(predFrame)
View(autoEncodedData)
predictions <- predict.autoencoder(encoder,as.matrix(data[,-10]),hidden.output = TRUE)
predFrame <- as.data.frame(predictions$X.output)
autoEncodedData <- tibble::add_column(predFrame,data$Select)
tsne <- Rtsne(as.matrix(unique(autoEncodedData)),perplexity = 20)
plot(tsne$Y,pch = 22, bg = c("blue","yellow"))
#Поменяли имя колонки (класс)
colnames(autoEncodedData)[which(names(autoEncodedData) == "data$Select")] <- "Select"
autoEncodedData$Select <- as.factor(autoEncodedData$Select)
View(train_data)
idx <- sample(1:n, n*split)
trainData <- autoEncodedData[idx, ]
testData <- autoEncodedData[-idx, ]
idx <- sample(1:n, n*split)
idx <- sample(1:n, n*0.7)
trainData <- autoEncodedData[idx, ]
testData <- autoEncodedData[-idx, ]
View(train_data)
View(autoEncodedData)
View(train_data)
View(trainData)
View(testData)
View(train_data)
testData$Sex <- as.numeric(as.factor(testData$Sex))
trainData$Sex <- as.numeric(as.factor(trainData$Sex))
encoder <- autoencode(as.matrix(data[,-10]),
N.hidden = 5,
epsilon = 0,
lambda = 0.0002,
beta = 6,
rho = 0.9,
unit.type = "tanh",
rescale.flag = TRUE)
predictions <- predict.autoencoder(encoder,as.matrix(data[,-10]),hidden.output = TRUE)
predFrame <- as.data.frame(predictions$X.output)
autoEncodedData <- tibble::add_column(predFrame,data$Select)
#Поменяли имя колонки (класс)
colnames(autoEncodedData)[which(names(autoEncodedData) == "data$Select")] <- "Select"
autoEncodedData$Select <- as.factor(autoEncodedData$Select)
#Разбили выборку на обучающую и тестирующую
idx <- sample(1:n, n*0.7)
trainData <- autoEncodedData[idx, ]
testData <- autoEncodedData[-idx, ]
#BAGGING
baggingModel <- bagging(Select ~.,data = trainData, mfinal = treeNumberBag, maxdepth = 5)
predictions <- predict.bagging(baggingModel, newdata = testData)
errBagging3 <- predictions$error
#BOOSTING
boostingModel <- boosting(Select ~.,data = trainData, mfinal = treeNumberBoost, maxdepth = 5)
predictions <- predict.boosting(boostingModel, newdata = testData)
errBoosting3 <- predictions$error
#SVM
svmModelLinear <- svm(Select ~ ., data = trainData, type = "C-classification", cost = 1, kernel = kernel)
predictions <- predict(svmModelLinear, testData[,-21])
myTable <- table(testData$"Select", predictions)
errSvm3 <- (myTable[2] + myTable[3]) / sum(myTable)
testData$Sex <- as.numeric(as.factor(testData$Sex))
trainData$Sex <- as.numeric(as.factor(trainData$Sex))
encoder <- autoencode(as.matrix(data[,-10]),
N.hidden = 15,
epsilon = 0,
lambda = 0.0002,
beta = 6,
rho = 0.9,
unit.type = "tanh",
rescale.flag = TRUE)
predictions <- predict.autoencoder(encoder,as.matrix(data[,-10]),hidden.output = TRUE)
predFrame <- as.data.frame(predictions$X.output)
autoEncodedData <- tibble::add_column(predFrame,data$Select)
tsne <- Rtsne(as.matrix(unique(autoEncodedData)),perplexity = 20)
plot(tsne$Y,pch = 22, bg = c("blue","yellow"))
#Поменяли имя колонки (класс)
colnames(autoEncodedData)[which(names(autoEncodedData) == "data$Select")] <- "Select"
autoEncodedData$Select <- as.factor(autoEncodedData$Select)
#Разбили выборку на обучающую и тестирующую
idx <- sample(1:n, n*0.7)
trainData <- autoEncodedData[idx, ]
testData <- autoEncodedData[-idx, ]
#BAGGING
baggingModel <- bagging(Select ~.,data = trainData, mfinal = treeNumberBag, maxdepth = 5)
predictions <- predict.bagging(baggingModel, newdata = testData)
errBagging2 <- predictions$error
#BOOSTING
boostingModel <- boosting(Select ~.,data = trainData, mfinal = treeNumberBoost, maxdepth = 5)
predictions <- predict.boosting(boostingModel, newdata = testData)
errBoosting2 <- predictions$error
#SVM
svmModelLinear <- svm(Select ~ ., data = trainData, type = "C-classification", cost = 1, kernel = kernel)
predictions <- predict(svmModelLinear, testData[,-21])
myTable <- table(testData$"Select", predictions)
errSvm <- (myTable[2] + myTable[3]) / sum(myTable)
testData$Sex <- as.numeric(as.factor(testData$Sex))
trainData$Sex <- as.numeric(as.factor(trainData$Sex))
encoder <- autoencode(as.matrix(data[,-10]),
N.hidden = 15,
epsilon = 0.5,
lambda = 0.0002,
beta = 6,
rho = 0.9,
unit.type = "tanh",
rescale.flag = TRUE)
predictions <- predict.autoencoder(encoder,as.matrix(data[,-10]),hidden.output = TRUE)
predFrame <- as.data.frame(predictions$X.output)
autoEncodedData <- tibble::add_column(predFrame,data$Select)
#Поменяли имя колонки (класс)
colnames(autoEncodedData)[which(names(autoEncodedData) == "data$Select")] <- "Select"
autoEncodedData$Select <- as.factor(autoEncodedData$Select)
#Разбили выборку на обучающую и тестирующую
idx <- sample(1:n, n*0.7)
trainData <- autoEncodedData[idx, ]
testData <- autoEncodedData[-idx, ]
#BAGGING
baggingModel <- bagging(Select ~.,data = trainData, mfinal = treeNumberBag, maxdepth = 5)
predictions <- predict.bagging(baggingModel, newdata = testData)
errBagging4 <- predictions$error
#BOOSTING
boostingModel <- boosting(Select ~.,data = trainData, mfinal = treeNumberBoost, maxdepth = 5)
predictions <- predict.boosting(boostingModel, newdata = testData)
errBoosting4 <- predictions$error
#SVM
svmModelLinear <- svm(Select ~ ., data = trainData, type = "C-classification", cost = 1, kernel = kernel)
predictions <- predict(svmModelLinear, testData[,-21])
myTable <- table(testData$"Select", predictions)
errSvm4 <- (myTable[2] + myTable[3]) / sum(myTable)
tsne <- Rtsne(as.matrix(unique(autoEncodedData)),perplexity = 20)
plot(tsne$Y,pch = 22, bg = c("blue","yellow"))
