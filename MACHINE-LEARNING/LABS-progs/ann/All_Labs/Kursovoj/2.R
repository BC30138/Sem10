library(e1071)
library(adabag)
library(rpart) 
library(Rtsne)
library(cluster)
library(glmnet)
library(autoencoder)

SourceData <- read.csv("C:/Users/anna_fox/Documents/UNIVERSE/4 курс/2 semester/Нейронные_сети/All_Labs/Kursovoj/Indian Liver Patient Dataset (ILPD).csv",stringsAsFactors = TRUE)

getBestKernel <- function(train, testCheck, kernels){
  test <- testCheck[,-10]
  errorList <- list()
  for (i in 1:length(kernels)){
    svmModelLinear <- svm(Select ~ ., data = train, type = "C-classification", cost = 1, kernel = kernels[i])  
    predictions <- predict(svmModelLinear, test)  
    myTable <- table(testCheck$"Select", predictions) 
    errorList[i] <- (myTable[2] + myTable[3]) / sum(myTable)
  }
  return(kernels[unlist(which.min(errorList))])
}

getBestKernel2 <- function(train, testCheck, kernels){
  test <- testCheck[,-10]
  errorList <- list()
  for (i in 1:length(kernels)){
    svmModelLinear <- svm(Select ~ ., data = train, type = "C-classification", cost = 1, kernel = kernels[i])  
    predictions <- predict(svmModelLinear, test)  
    myTable <- table(testCheck$"Select", predictions) 
    errorList[i] <- (myTable[2] + myTable[3]) / sum(myTable)
  }
  return(errorList)
}

getBestMFinalBagging <- function(train,test,mfinal){
  resList <- list()
  for (i in 1:length(mfinal)) {
    model <- bagging(Select ~.,data = train, mfinal = mfinal[i], maxdepth = 5)
    predictions <- predict.bagging(model, newdata = test)  
    resList[i] <- predictions$error 
  }
  return(resList)
}

getBestMFinalBoosting <- function(train,test,mfinal){
  resList <- list()
  for (i in 1:length(mfinal)) {
    model <- boosting(Select ~.,data = train, mfinal = mfinal[i], maxdepth = 5)
    predictions <- predict.boosting(model, newdata = test)  
    resList[i] <- predictions$error 
  }
  return(resList)
}
############################################################################################
##Visual
############################################################################################
SourceData <- SourceData[,-10]
n <- dim(SourceData)[1]

idx <- sample(1:n, n*0.7)
trainData <- SourceData[idx, ]
testData <- SourceData[-idx, ]

train_data <- unique(trainData)
train_data$Sex <- as.numeric(as.factor(train_data$Sex))
tsne <- Rtsne(as.matrix(train_data))
plot(tsne$Y,pch = 22, bg = c("blue","yellow"))

############################################################################################
##SVM
############################################################################################

#Нашли ядро с наименьшей ошибкой
kernel <- getBestKernel(trainData,testData,c("linear","radial","sigmoid","polynomial"))
#kkkkkk <- getBestKernel2(trainData,testData,c("linear","radial","sigmoid","polynomial"))

svmModelLinear <- svm(Select ~ ., data = trainData, type = "C-classification", cost = 1, kernel = kernel)  
predictions <- predict(svmModelLinear, testData[,-10])  
myTable <- table(testData$"Select", predictions) 
errSvm <- (myTable[2] + myTable[3]) / sum(myTable)


############################################################################################
##BAGGING
############################################################################################
mfinal <- seq(1,6,5)
trainData$Select <- as.factor(trainData$Select)
resList <- getBestMFinalBagging(trainData,testData,mfinal)

plot(mfinal,resList,    
     col = "red",
     xlab = "Number of trees",
     ylab = "Classification error")

#Нашли количество деревьев с наименьшей ошибкой
treeNumberBag <- mfinal[unlist(which.min(resList))]

baggingModel <- bagging(Select ~.,data = trainData, mfinal = treeNumberBag, maxdepth = 5)
predictions <- predict.bagging(baggingModel, newdata = testData)  
errBagging <- predictions$error

############################################################################################
##BOOSTING
############################################################################################
resList <- getBestMFinalBoosting(trainData,testData,mfinal)

plot(mfinal,resList,    
     col = "red",
     xlab = "Number of trees",
     ylab = "Classification error")

#Нашли количество деревьев с наименьшей ошибкой
treeNumberBoost <- mfinal[unlist(which.min(resList))]

boostingModel <- boosting(Select ~.,data = trainData, mfinal = treeNumberBoost, maxdepth = 5)
predictions <- predict.boosting(boostingModel, newdata = testData)  
errBoosting <- predictions$error

############################################################################################
##CLUSTER
############################################################################################
cl <- clara(SourceData[,-10], 2,metric = c("euclidean"))
t <- table(cl$clustering,SourceData$Select)
errClara <- sum(diag(t))/sum(t)
plot(cl)

data <- SourceData
data$Sex <- as.numeric(as.factor(data$Sex))
kkk <- kmeans(data[,-10],2,iter.max = 10)
t <- table(kkk$cluster,SourceData$Select)
errKMeans <- sum(diag(t))/sum(t)


# prepare hierarchical cluster
hc = hclust(dist(SourceData[,-10]))
# very simple dendrogram
plot(hc,hang=-1)
groups<-cutree(hc,k=2)
#rect.hclust(hc,k=2,border = "red")
t<-table(groups,SourceData[,10])
(sum(diag(t))/sum(t))

############################################################################################
##LASSO
############################################################################################
x <- as.matrix(data[,-10])
y <- data[,10]
glm = glmnet(x, y, family = "binomial", alpha = 1)
lassoResult <- as.matrix(glm$beta)

############################################################################################
##AUTOENCODER 
############################################################################################

testData$Sex <- as.numeric(as.factor(testData$Sex))
trainData$Sex <- as.numeric(as.factor(trainData$Sex))
encoder <- autoencode(as.matrix(data[,-10]),
                      N.hidden = 15,
                      epsilon = 0,
                      lambda = 0.0002,
                      beta = 6,
                      rho = 0.9,
                      unit.type = "tanh",
                      rescale.flag = TRUE)

predictions <- predict.autoencoder(encoder,as.matrix(data[,-10]),hidden.output = TRUE)
predFrame <- as.data.frame(predictions$X.output)
autoEncodedData <- tibble::add_column(predFrame,data$Select)

tsne <- Rtsne(as.matrix(unique(autoEncodedData)),perplexity = 20)
plot(tsne$Y,pch = 22, bg = c("blue","yellow"))

#Поменяли имя колонки (класс)
colnames(autoEncodedData)[which(names(autoEncodedData) == "data$Select")] <- "Select"
autoEncodedData$Select <- as.factor(autoEncodedData$Select)

#Разбили выборку на обучающую и тестирующую
idx <- sample(1:n, n*0.7)
trainData <- autoEncodedData[idx, ]
testData <- autoEncodedData[-idx, ]

#BAGGING
baggingModel <- bagging(Select ~.,data = trainData, mfinal = treeNumberBag, maxdepth = 5)
predictions <- predict.bagging(baggingModel, newdata = testData)  
errBagging2 <- predictions$error

#BOOSTING
boostingModel <- boosting(Select ~.,data = trainData, mfinal = treeNumberBoost, maxdepth = 5)
predictions <- predict.boosting(boostingModel, newdata = testData)  
errBoosting2 <- predictions$error

#SVM
svmModelLinear <- svm(Select ~ ., data = trainData, type = "C-classification", cost = 1, kernel = kernel)  
predictions <- predict(svmModelLinear, testData[,-21])  
myTable <- table(testData$"Select", predictions) 
errSvm <- (myTable[2] + myTable[3]) / sum(myTable)


############################################################################################
##AUTOENCODER
############################################################################################

testData$Sex <- as.numeric(as.factor(testData$Sex))
trainData$Sex <- as.numeric(as.factor(trainData$Sex))
encoder <- autoencode(as.matrix(data[,-10]),
                      N.hidden = 5,
                      epsilon = 0,
                      lambda = 0.0002,
                      beta = 6,
                      rho = 0.9,
                      unit.type = "tanh",
                      rescale.flag = TRUE)

predictions <- predict.autoencoder(encoder,as.matrix(data[,-10]),hidden.output = TRUE)
predFrame <- as.data.frame(predictions$X.output)
autoEncodedData <- tibble::add_column(predFrame,data$Select)

#Поменяли имя колонки (класс)
colnames(autoEncodedData)[which(names(autoEncodedData) == "data$Select")] <- "Select"
autoEncodedData$Select <- as.factor(autoEncodedData$Select)

#Разбили выборку на обучающую и тестирующую
idx <- sample(1:n, n*0.7)
trainData <- autoEncodedData[idx, ]
testData <- autoEncodedData[-idx, ]

#BAGGING
baggingModel <- bagging(Select ~.,data = trainData, mfinal = treeNumberBag, maxdepth = 5)
predictions <- predict.bagging(baggingModel, newdata = testData)  
errBagging3 <- predictions$error

#BOOSTING
boostingModel <- boosting(Select ~.,data = trainData, mfinal = treeNumberBoost, maxdepth = 5)
predictions <- predict.boosting(boostingModel, newdata = testData)  
errBoosting3 <- predictions$error

#SVM
svmModelLinear <- svm(Select ~ ., data = trainData, type = "C-classification", cost = 1, kernel = kernel)  
predictions <- predict(svmModelLinear, testData[,-21])  
myTable <- table(testData$"Select", predictions) 
errSvm3 <- (myTable[2] + myTable[3]) / sum(myTable)

############################################################################################
##AUTOENCODER
############################################################################################
testData$Sex <- as.numeric(as.factor(testData$Sex))
trainData$Sex <- as.numeric(as.factor(trainData$Sex))
encoder <- autoencode(as.matrix(data[,-10]),
                      N.hidden = 15,
                      epsilon = 0.5,
                      lambda = 0.0002,
                      beta = 6,
                      rho = 0.9,
                      unit.type = "tanh",
                      rescale.flag = TRUE)

predictions <- predict.autoencoder(encoder,as.matrix(data[,-10]),hidden.output = TRUE)
predFrame <- as.data.frame(predictions$X.output)
autoEncodedData <- tibble::add_column(predFrame,data$Select)

#Поменяли имя колонки (класс)
colnames(autoEncodedData)[which(names(autoEncodedData) == "data$Select")] <- "Select"
autoEncodedData$Select <- as.factor(autoEncodedData$Select)

#Разбили выборку на обучающую и тестирующую
idx <- sample(1:n, n*0.7)
trainData <- autoEncodedData[idx, ]
testData <- autoEncodedData[-idx, ]

#BAGGING
baggingModel <- bagging(Select ~.,data = trainData, mfinal = treeNumberBag, maxdepth = 5)
predictions <- predict.bagging(baggingModel, newdata = testData)  
errBagging4 <- predictions$error

#BOOSTING
boostingModel <- boosting(Select ~.,data = trainData, mfinal = treeNumberBoost, maxdepth = 5)
predictions <- predict.boosting(boostingModel, newdata = testData)  
errBoosting4 <- predictions$error

#SVM
svmModelLinear <- svm(Select ~ ., data = trainData, type = "C-classification", cost = 1, kernel = kernel)  
predictions <- predict(svmModelLinear, testData[,-21])  
myTable <- table(testData$"Select", predictions) 
errSvm4 <- (myTable[2] + myTable[3]) / sum(myTable)
