#sp.tr1 <- prune.misclass(sp.tr)
#варианты кода внизу не сработали
#Запустите процедуру “cost-complexity prunning” с выбором параметра k по умолчанию, method = ’misclass’
#sp.tr1 <- prune.tree(sp.tr, k = NULL, method = "misclass")
#bc.tr2 <- prune.tree(bc.tr)
#sp.tr1 <- cv.tree(sp.tr, prune.tree)
#sp.tr1 <- prune.misclass(sp.tr)
draw.tree(sp.tr1, cex = 0.7)
sp.tr1 <- prune.misclass(sp.tr, best = 8)
#sp.tr1 <- prune.misclass(sp.tr)
#варианты кода внизу не сработали
#Запустите процедуру “cost-complexity prunning” с выбором параметра k по умолчанию, method = ’misclass’
#sp.tr1 <- prune.tree(sp.tr, k = NULL, method = "misclass")
#bc.tr2 <- prune.tree(bc.tr)
#sp.tr1 <- cv.tree(sp.tr, prune.tree)
#sp.tr1 <- prune.misclass(sp.tr)
draw.tree(sp.tr1, cex = 0.7)
sp.tr1 <- prune.misclass(sp.tr, best = 4)
#sp.tr1 <- prune.misclass(sp.tr)
#варианты кода внизу не сработали
#Запустите процедуру “cost-complexity prunning” с выбором параметра k по умолчанию, method = ’misclass’
#sp.tr1 <- prune.tree(sp.tr, k = NULL, method = "misclass")
#bc.tr2 <- prune.tree(bc.tr)
#sp.tr1 <- cv.tree(sp.tr, prune.tree)
#sp.tr1 <- prune.misclass(sp.tr)
draw.tree(sp.tr1, cex = 0.7)
#install.packages("tree")
#install.packages("maptree")
#install.packages("mlbench")
#install.packages("DAAG")
library(tree)
library(maptree)
library(DAAG)
data(spam7)
m <- dim(spam7)[1]
sp.tr <- tree(yesno ~., spam7)
draw.tree(sp.tr, cex = 0.7)
cv.tree(sp.tr)
plot(cv.tree(sp.tr))
sp.tr1 <- prune.misclass(sp.tr, best = 5)
#sp.tr1 <- prune.misclass(sp.tr)
#варианты кода внизу не сработали
#Запустите процедуру “cost-complexity prunning” с выбором параметра k по умолчанию, method = ’misclass’
#sp.tr1 <- prune.tree(sp.tr, k = NULL, method = "misclass")
#bc.tr2 <- prune.tree(bc.tr)
#sp.tr1 <- cv.tree(sp.tr, prune.tree)
#sp.tr1 <- prune.misclass(sp.tr)
draw.tree(sp.tr1, cex = 0.7)
sp.tr1 <- prune.misclass(sp.tr, best = 6)
#sp.tr1 <- prune.misclass(sp.tr)
#варианты кода внизу не сработали
#Запустите процедуру “cost-complexity prunning” с выбором параметра k по умолчанию, method = ’misclass’
#sp.tr1 <- prune.tree(sp.tr, k = NULL, method = "misclass")
#bc.tr2 <- prune.tree(bc.tr)
#sp.tr1 <- cv.tree(sp.tr, prune.tree)
#sp.tr1 <- prune.misclass(sp.tr)
draw.tree(sp.tr1, cex = 0.7)
#без указания best не запускается
sp.tr1 <- prune.misclass(sp.tr, best = 7)
#sp.tr1 <- prune.misclass(sp.tr)
#варианты кода внизу не сработали
#Запустите процедуру “cost-complexity prunning” с выбором параметра k по умолчанию, method = ’misclass’
#sp.tr1 <- prune.tree(sp.tr, k = NULL, method = "misclass")
#bc.tr2 <- prune.tree(bc.tr)
#sp.tr1 <- cv.tree(sp.tr, prune.tree)
#sp.tr1 <- prune.misclass(sp.tr)
draw.tree(sp.tr1, cex = 0.7)
sp.tr1 <- prune.misclass(sp.tr, best = 6)
#sp.tr1 <- prune.misclass(sp.tr)
#варианты кода внизу не сработали
#Запустите процедуру “cost-complexity prunning” с выбором параметра k по умолчанию, method = ’misclass’
#sp.tr1 <- prune.tree(sp.tr, k = NULL, method = "misclass")
#bc.tr2 <- prune.tree(bc.tr)
#sp.tr1 <- cv.tree(sp.tr, prune.tree)
#sp.tr1 <- prune.misclass(sp.tr)
draw.tree(sp.tr1, cex = 0.7)
sp.tr1 <- prune.misclass(sp.tr, best = 5)
#sp.tr1 <- prune.misclass(sp.tr)
#варианты кода внизу не сработали
#Запустите процедуру “cost-complexity prunning” с выбором параметра k по умолчанию, method = ’misclass’
#sp.tr1 <- prune.tree(sp.tr, k = NULL, method = "misclass")
#bc.tr2 <- prune.tree(bc.tr)
#sp.tr1 <- cv.tree(sp.tr, prune.tree)
#sp.tr1 <- prune.misclass(sp.tr)
draw.tree(sp.tr1, cex = 0.7)
library(tree)
library(maptree)
library(mlbench)
data(Glass)
m <- dim(Glass)[1]
Glass <- Glass[,-1]
bc.tr <- tree(Type ~., Glass)
draw.tree(bc.tr, cex = 0.7)
#обрезаем дерево от узла 26 (108, 31), так как оба выходящих узла принадлежат классу 1 (классу 1, классу 2)
bc.tr1 <- snip.tree(bc.tr, nodes = c(26, 108, 31))
draw.tree(bc.tr1, cex = 0.7)
bc.tr2 <- prune.tree(bc.tr, k = 10)
#bc.tr2 <- prune.tree(bc.tr)
draw.tree(bc.tr2, cex = 0.7)
library(tree)
library(maptree)
library(mlbench)
data(Glass)
m <- dim(Glass)[1]
Glass <- Glass[,-1]
bc.tr <- tree(Type ~., Glass)
draw.tree(bc.tr, cex = 0.7)
library(tree)
library(maptree)
library(mlbench)
data(Glass)
m <- dim(Glass)[1]
Glass <- Glass[,-1]
bc.tr <- tree(Type ~., Glass)
draw.tree(bc.tr, cex = 0.7)
library(tree)
library(maptree)
library(mlbench)
data(Glass)
m <- dim(Glass)[1]
Glass <- Glass[,-1]
bc.tr <- tree(Type ~., Glass)
draw.tree(bc.tr, cex = 0.7)
#обрезаем дерево от узла 26 (108, 31), так как оба выходящих узла принадлежат классу 1 (классу 1, классу 2)
bc.tr1 <- snip.tree(bc.tr, nodes = c(26, 108, 31))
draw.tree(bc.tr1, cex = 0.7)
bc.tr1 <- snip.tree(bc.tr, nodes = c(26, 108, 31))
draw.tree(bc.tr1, cex = 0.7)
library(mlbench)
data(Glass)
m <- dim(Glass)[1]
Glass <- Glass[,-1]
bc.tr <- tree(Type ~., Glass)
draw.tree(bc.tr, cex = 0.7)
bc.tr
summary(bc.tr)
#обрезаем дерев
library(tree)
library(maptree)
library(mlbench)
data(Glass)
m <- dim(Glass)[1]
#Glass <- Glass[,-1]
bc.tr <- tree(Type ~., Glass)
draw.tree(bc.tr, cex = 0.7)
bc.tr
summary(bc.tr)
library(tree)
library(maptree)
library(mlbench)
data(Glass)
m <- dim(Glass)[1]
Glass <- Glass[,-1]
bc.tr <- tree(Type ~., Glass)
draw.tree(bc.tr, cex = 0.7)
bc.tr
summary(bc.tr)
library(tree)
library(maptree)
library(mlbench)
data(Glass)
m <- dim(Glass)[1]
Glass <- Glass[,-1]
bc.tr <- tree(Type ~., Glass)
draw.tree(bc.tr, cex = 0.7)
bc.tr
summary(bc.tr)
bc.tr1 <- snip.tree(bc.tr, nodes = c(31, 103))
draw.tree(bc.tr1, cex = 0.7)
bc.tr2 <- prune.tree(bc.tr, k = 10)
#bc.tr2 <- prune.tree(bc.tr)
draw.tree(bc.tr2, cex = 0.7)
bc.tr2 <- prune.tree(bc.tr, k = 10)
#bc.tr2 <- prune.tree(bc.tr)
draw.tree(bc.tr2, cex = 0.7)
bc.tr2 <- prune.tree(bc.tr, k = 10)
#bc.tr2 <- prune.tree(bc.tr)
draw.tree(bc.tr2, cex = 0.7)
library(tree)
library(maptree)
library(mlbench)
data(Glass)
m <- dim(Glass)[1]
Glass <- Glass[,-1]
bc.tr <- tree(Type ~., Glass)
draw.tree(bc.tr, cex = 0.7)
bc.tr
summary(bc.tr)
#обрезаем дерево от узла 103, 31 так как оба выходящих узла принадлежат классу 1,2
bc.tr1 <- snip.tree(bc.tr, nodes = c(31, 103))
draw.tree(bc.tr1, cex = 0.7)
bc.tr2 <- prune.tree(bc.tr, k = 10)
#bc.tr2 <- prune.tree(bc.tr)
draw.tree(bc.tr2, cex = 0.7)
example <- data.frame("RI" = 1.516,"Na" = 11.7, "Mg" = 1.01, "Al" = 1.19, "Si" = 72.59, "K" = 0.43,"Ca" = 11.44, "Ba" = 0.02, "Fe" = 0.1, "Type" = "")
predict(bc.tr, example)
View(example)
View(example)
which.is.max(example)
which.max(example)
which.min(example)
z<-apply(example,1,which.max)
names(example)[z]
View(Glass)
z<-apply(Glass,1,which.max)
names(Glass)[z]
a <-names(Glass)[z]
a[1]
library(e1071)
library(adabag)
library(rpart)
library(Rtsne)
library(cluster)
library(glmnet)
library(autoencoder)
SourceData <- read.csv("C:/Users/anna_fox/Documents/UNIVERSE/4 курс/2 semester/Нейронные_сети/All_Labs/Kursovoj/Indian Liver Patient Dataset (ILPD).csv",stringsAsFactors = TRUE)
getBestKernel <- function(train, testCheck, kernels){
test <- testCheck[,-10]
errorList <- list()
for (i in 1:length(kernels)){
svmModelLinear <- svm(Select ~ ., data = train, type = "C-classification", cost = 1, kernel = kernels[i])
predictions <- predict(svmModelLinear, test)
myTable <- table(testCheck$"Select", predictions)
errorList[i] <- (myTable[2] + myTable[3]) / sum(myTable)
}
return(kernels[unlist(which.min(errorList))])
}
getBestMFinalBagging <- function(train,test,mfinal){
resList <- list()
for (i in 1:length(mfinal)) {
model <- bagging(Select ~.,data = train, mfinal = mfinal[i], maxdepth = 5)
predictions <- predict.bagging(model, newdata = test)
resList[i] <- predictions$error
}
return(resList)
}
getBestMFinalBoosting <- function(train,test,mfinal){
resList <- list()
for (i in 1:length(mfinal)) {
model <- boosting(Select ~.,data = train, mfinal = mfinal[i], maxdepth = 5)
predictions <- predict.boosting(model, newdata = test)
resList[i] <- predictions$error
}
return(resList)
}
############################################################################################
##Visual
############################################################################################
SourceData <- SourceData[,-10]
n <- dim(SourceData)[1]
idx <- sample(1:n, n*0.7)
trainData <- SourceData[idx, ]
testData <- SourceData[-idx, ]
train_data <- unique(trainData)
train_data$Sex <- as.numeric(as.factor(train_data$Sex))
tsne <- Rtsne(as.matrix(train_data))
plot(tsne$Y,pch = 22, bg = c("blue","yellow"))
cl <- clara(SourceData[,-10], 2,metric = c("euclidean"))
t <- table(cl$clustering,SourceData$Select)
errClara <- sum(diag(t))/sum(t)
plot(cl)
data$Sex <- as.numeric(as.factor(data$Sex))
kkk <- kmeans(data[,-10],2,iter.max = 10)
t <- table(kkk$cluster,SourceData$Select)
errKMeans <- sum(diag(t))/sum(t)
dendogramma <- agnes(SourceData)
plot(dendogramma)
cl <- clara(SourceData[,-10], 2,metric = c("euclidean"))
t <- table(cl$clustering,SourceData$Select)
errClara <- sum(diag(t))/sum(t)
plot(cl)
plot(agnes(SourceData))
View(SourceData)
cl <- clara(SourceData[,-c(2,10)], 2,metric = c("euclidean"))
t <- table(cl$clustering,SourceData$Select)
errClara <- sum(diag(t))/sum(t)
plot(cl)
data <- SourceData
data$Sex <- as.numeric(as.factor(data$Sex))
kkk <- kmeans(data[,-10],2,iter.max = 10)
t <- table(kkk$cluster,SourceData$Select)
errKMeans <- sum(diag(t))/sum(t)
dendogramma <- agnes(SourceData)
plot(dendogramma)
cl <- clara(SourceData[,-c(1,10)], 2,metric = c("euclidean"))
t <- table(cl$clustering,SourceData$Select)
errClara <- sum(diag(t))/sum(t)
plot(cl)
data <- SourceData
data$Sex <- as.numeric(as.factor(data$Sex))
kkk <- kmeans(data[,-10],2,iter.max = 10)
t <- table(kkk$cluster,SourceData$Select)
errKMeans <- sum(diag(t))/sum(t)
dendogramma <- agnes(SourceData)
plot(dendogramma)
dendogramma <- agnes(SourceData[,-10])
plot(dendogramma)
dendogramma <- agnes(SourceData[,-c(1,10)])
plot(dendogramma)
SourceData <- SourceData[,-10]
n <- dim(SourceData)[1]
idx <- sample(1:n, n*0.7)
trainData <- SourceData[idx, ]
testData <- SourceData[-idx, ]
train_data <- unique(trainData)
train_data$Sex <- as.numeric(as.factor(train_data$Sex))
tsne <- Rtsne(as.matrix(train_data))
plot(tsne$Y,pch = 22, bg = c("blue","yellow"))
############################################################################################
x <- as.matrix(data[,-10])
y <- data[,10]
glm = glmnet(x, y, family = "binomial", alpha = 1)
lassoResult <- as.matrix(glm$beta)
View(lassoResult)
View(glm)
testData$Sex <- as.numeric(as.factor(testData$Sex))
trainData$Sex <- as.numeric(as.factor(trainData$Sex))
encoder <- autoencode(as.matrix(data[,-10]),
N.hidden = 5,
epsilon = 0,
lambda = 0.0002,
beta = 6,
rho = 0.9,
unit.type = "tanh",
rescale.flag = TRUE)
predictions <- predict.autoencoder(encoder,as.matrix(data[,-10]),hidden.output = TRUE)
predFrame <- as.data.frame(predictions$X.output)
autoEncodedData <- tibble::add_column(predFrame,data$Select)
#Поменяли имя колонки (класс)
colnames(autoEncodedData)[which(names(autoEncodedData) == "data$Select")] <- "Select"
autoEncodedData$Select <- as.factor(autoEncodedData$Select)
View(autoEncodedData)
library(e1071)
library(adabag)
library(rpart)
library(Rtsne)
library(cluster)
library(glmnet)
library(autoencoder)
SourceData <- read.csv("C:/Users/anna_fox/Documents/UNIVERSE/4 курс/2 semester/Нейронные_сети/All_Labs/Kursovoj/Indian Liver Patient Dataset (ILPD).csv",stringsAsFactors = TRUE)
getBestKernel <- function(train, testCheck, kernels){
test <- testCheck[,-10]
errorList <- list()
for (i in 1:length(kernels)){
svmModelLinear <- svm(Select ~ ., data = train, type = "C-classification", cost = 1, kernel = kernels[i])
predictions <- predict(svmModelLinear, test)
myTable <- table(testCheck$"Select", predictions)
errorList[i] <- (myTable[2] + myTable[3]) / sum(myTable)
}
return(kernels[unlist(which.min(errorList))])
}
getBestMFinalBagging <- function(train,test,mfinal){
resList <- list()
for (i in 1:length(mfinal)) {
model <- bagging(Select ~.,data = train, mfinal = mfinal[i], maxdepth = 5)
predictions <- predict.bagging(model, newdata = test)
resList[i] <- predictions$error
}
return(resList)
}
getBestMFinalBoosting <- function(train,test,mfinal){
resList <- list()
for (i in 1:length(mfinal)) {
model <- boosting(Select ~.,data = train, mfinal = mfinal[i], maxdepth = 5)
predictions <- predict.boosting(model, newdata = test)
resList[i] <- predictions$error
}
return(resList)
}
############################################################################################
##Visual
############################################################################################
SourceData <- SourceData[,-10]
n <- dim(SourceData)[1]
idx <- sample(1:n, n*0.7)
trainData <- SourceData[idx, ]
testData <- SourceData[-idx, ]
train_data <- unique(trainData)
train_data$Sex <- as.numeric(as.factor(train_data$Sex))
tsne <- Rtsne(as.matrix(train_data))
plot(tsne$Y,pch = 22, bg = c("blue","yellow"))
############################################################################################
##SVM
############################################################################################
#Нашли ядро с наименьшей ошибкой
kernel <- getBestKernel(trainData,testData,c("linear","radial","sigmoid","polynomial"))
svmModelLinear <- svm(Select ~ ., data = trainData, type = "C-classification", cost = 1, kernel = kernel)
predictions <- predict(svmModelLinear, testData[,-10])
myTable <- table(testData$"Select", predictions)
errSvm <- (myTable[2] + myTable[3]) / sum(myTable)
############################################################################################
##BAGGING
############################################################################################
mfinal <- seq(1,6,5)
trainData$Select <- as.factor(trainData$Select)
resList <- getBestMFinalBagging(trainData,testData,mfinal)
plot(mfinal,resList,
col = "red",
xlab = "Number of trees",
ylab = "Classification error")
#Нашли количество деревьев с наименьшей ошибкой
treeNumberBag <- mfinal[unlist(which.min(resList))]
baggingModel <- bagging(Select ~.,data = trainData, mfinal = treeNumberBag, maxdepth = 5)
predictions <- predict.bagging(baggingModel, newdata = testData)
errBagging <- predictions$error
############################################################################################
##BOOSTING
############################################################################################
resList <- getBestMFinalBoosting(trainData,testData,mfinal)
plot(mfinal,resList,
col = "red",
xlab = "Number of trees",
ylab = "Classification error")
#Нашли количество деревьев с наименьшей ошибкой
treeNumberBoost <- mfinal[unlist(which.min(resList))]
boostingModel <- boosting(Select ~.,data = trainData, mfinal = treeNumberBoost, maxdepth = 5)
predictions <- predict.boosting(boostingModel, newdata = testData)
errBoosting <- predictions$error
############################################################################################
##CLUSTER
############################################################################################
cl <- clara(SourceData[,-c(1,10)], 2,metric = c("euclidean"))
t <- table(cl$clustering,SourceData$Select)
errClara <- sum(diag(t))/sum(t)
plot(cl)
data <- SourceData
data$Sex <- as.numeric(as.factor(data$Sex))
kkk <- kmeans(data[,-10],2,iter.max = 10)
t <- table(kkk$cluster,SourceData$Select)
errKMeans <- sum(diag(t))/sum(t)
dendogramma <- agnes(SourceData[,-c(1,10)])
plot(dendogramma)
##LASSO
############################################################################################
x <- as.matrix(data[,-10])
y <- data[,10]
glm = glmnet(x, y, family = "binomial", alpha = 1)
lassoResult <- as.matrix(glm$beta)
############################################################################################
##AUTOENCODER
############################################################################################
testData$Sex <- as.numeric(as.factor(testData$Sex))
trainData$Sex <- as.numeric(as.factor(trainData$Sex))
encoder <- autoencode(as.matrix(data[,-10]),
N.hidden = 20,
epsilon = 0,
lambda = 0.0002,
beta = 6,
rho = 0.9,
unit.type = "tanh",
rescale.flag = TRUE)
predictions <- predict.autoencoder(encoder,as.matrix(data[,-10]),hidden.output = TRUE)
predFrame <- as.data.frame(predictions$X.output)
autoEncodedData <- tibble::add_column(predFrame,data$Select)
tsne <- Rtsne(as.matrix(unique(autoEncodedData)),perplexity = 20)
plot(tsne$Y,pch = 22, bg = c("blue","yellow"))
#Поменяли имя колонки (класс)
colnames(autoEncodedData)[which(names(autoEncodedData) == "data$Select")] <- "Select"
autoEncodedData$Select <- as.factor(autoEncodedData$Select)
#Разбили выборку на обучающую и тестирующую
idx <- sample(1:n, n*split)
trainData <- autoEncodedData[idx, ]
testData <- autoEncodedData[-idx, ]
#BAGGING
baggingModel <- bagging(Select ~.,data = trainData, mfinal = treeNumberBag, maxdepth = 5)
predictions <- predict.bagging(baggingModel, newdata = testData)
errBagging2 <- predictions$error
#BOOSTING
boostingModel <- boosting(Select ~.,data = trainData, mfinal = treeNumberBoost, maxdepth = 5)
predictions <- predict.boosting(boostingModel, newdata = testData)
errBoosting2 <- predictions$error
#SVM
svmModelLinear <- svm(Select ~ ., data = trainData, type = "C-classification", cost = 1, kernel = kernel)
predictions <- predict(svmModelLinear, testData[,-21])
myTable <- table(testData$"Select", predictions)
errSvm <- (myTable[2] + myTable[3]) / sum(myTable)
############################################################################################
##AUTOENCODER
############################################################################################
testData$Sex <- as.numeric(as.factor(testData$Sex))
trainData$Sex <- as.numeric(as.factor(trainData$Sex))
encoder <- autoencode(as.matrix(data[,-10]),
N.hidden = 5,
epsilon = 0,
lambda = 0.0002,
beta = 6,
rho = 0.9,
unit.type = "tanh",
rescale.flag = TRUE)
predictions <- predict.autoencoder(encoder,as.matrix(data[,-10]),hidden.output = TRUE)
predFrame <- as.data.frame(predictions$X.output)
autoEncodedData <- tibble::add_column(predFrame,data$Select)
#Поменяли имя колонки (класс)
colnames(autoEncodedData)[which(names(autoEncodedData) == "data$Select")] <- "Select"
autoEncodedData$Select <- as.factor(autoEncodedData$Select)
#Разбили выборку на обучающую и тестирующую
idx <- sample(1:n, n*split)
trainData <- autoEncodedData[idx, ]
testData <- autoEncodedData[-idx, ]
#BAGGING
baggingModel <- bagging(Select ~.,data = trainData, mfinal = treeNumberBag, maxdepth = 5)
predictions <- predict.bagging(baggingModel, newdata = testData)
errBagging3 <- predictions$error
#BOOSTING
boostingModel <- boosting(Select ~.,data = trainData, mfinal = treeNumberBoost, maxdepth = 5)
predictions <- predict.boosting(boostingModel, newdata = testData)
errBoosting3 <- predictions$error
#SVM
svmModelLinear <- svm(Select ~ ., data = trainData, type = "C-classification", cost = 1, kernel = kernel)
predictions <- predict(svmModelLinear, testData[,-21])
myTable <- table(testData$"Select", predictions)
errSvm3 <- (myTable[2] + myTable[3]) / sum(myTable)
